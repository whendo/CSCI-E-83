{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107f587f",
   "metadata": {},
   "source": [
    "# Chapter 30 \n",
    "# Hierarchical Bayesian Models\n",
    "\n",
    "## Introduction     \n",
    "\n",
    "In this chapter extend Bayesian methods to the class of hierarchical models. Up until now, we have focused on pooled models, which pool all data when estimating model parameters. However, as we have seen already, many real-world datasets have discrete categories or strata. For example, a chemist might measure a reaction rate at several discrete levels of pH and temperature, or the problem might involve demographic categories or different geographic regions.    \n",
    "\n",
    "A pooled model simply computes model coefficients that give the best fit to the overall dataset. You can think of the pooled model as having a flat structure, in the sense that all model coefficients are at the same level. The pooling maximizes statistical power by using all available data to fit a model but clearly lacks flexibility if the classes in the data have different behaviors. \n",
    "\n",
    "Another extreme alternative is to use an unpooled model. For each category a separate and independent unpooled model is fit. While this approach maximizes flexibility, the variance of each of the, possibly many, models will be larger. As with the pooled model you can think of unpooled models as having a flat structure.       \n",
    "\n",
    "The idea of hierarchical models is to find the best of both worlds with an approach that is between the extremes of pooled and unpooled models. Hierarchical models are constructed by creating a hierarchy of hyperpriors. The Hyperpriors represent the prior information on the pooled data. The hyperpriors act as priors for the specific cases. In this way, the hierarchical models occupy a middle ground between pooled and unpooled models.      \n",
    "\n",
    "In this chapter, we focus on three key points:   \n",
    "1.\tPooled vs. unpooled models.  \n",
    "2.\tDefining hierarchical models through hyperpriors and priors.   \n",
    "3.\tEvaluation and comparison of Bayesian models.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394427c8",
   "metadata": {},
   "source": [
    "## Danger of Radon in Homes   \n",
    "\n",
    "The unstable isotope Radon-222 is an invisible and orderless gas that is the product of a nuclear decay chain of Uranium 238. The decay chain that creates Radon-222 is shown in the figure below.    \n",
    "\n",
    "\n",
    "<img src=\"../images/UtoRn.png\" alt=\"Drawing\" style=\"width:400px; height:600px\"/>\n",
    "<center> Decay chain of Uranium-238 to Radon-222 <center> \n",
    "<center> Credit: <a href=\"https://en.wikipedia.org/wiki/Radon-222\">Wilipedia Radon-222 article!</a> <center>\n",
    "    \n",
    "Depending on rock and soil type, trace amounts of Uranium-238 are present. Depending on the chemistry, Uranium is [effectively transported by groundwater](https://www.nrc.gov/docs/ML0931/ML093160829.pdf). Hence the decay products, including Radon-222, are dispersed in the environment. Under certain \n",
    "    \n",
    "The health risk of Radon infiltration into homes is [well-documented](https://www.epa.gov/radon/health-risk-radon). Radon is the most prevalent cause of lung cancer among US nonsmokers. In the outdoors, Radon-222 does not accumulate in dangerous quantities. However, within poorly ventilated buildings, the Radon gas cannot dissipate and can accumulate to dangerous levels. Since the Radon molecule is heavy, it tends to accumulate in low areas, such as basements. Further, houses connected to wells or in contact with groundwater are more likely to accumulate dangerous levels of Radon. The diagram below illustrates these points.      \n",
    "\n",
    "<img src=\"../images/Radon_Home_Diagram.png\" alt=\"Drawing\" style=\"width:550px; height:400px\"/>\n",
    "<center> Common ways Radon-222 enters homes <center> \n",
    "<center> Credit: <a href=\"https://matracking.ehs.state.ma.us/Environmental-Data/radon/radon_lessons.html\">Massachusetts Department of Public Health!</a> <center>\n",
    "\n",
    "As has already been mentioned the danger from Randon changes with soil and rock composition as well as groundwater chemistry. These facts mean that the risk changes significantly with geographic location. An example the map below shows the Radon risk for the counties of Iowa and Minnesota. Notice that the risk varies considerably in space.      \n",
    "\n",
    "<img src=\"../images/MN_IA_CountryRadonMap.jpg\" alt=\"Drawing\" style=\"width:500px; height:600px\"/>\n",
    "<center> Map of Radon in homes for counties in IA and MN <center> \n",
    "<center> Credit: <a href=\"http://employees.csbsju.edu/dsteck/mnradon/\">Minnesota Radon Project!</a> <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c00ae0",
   "metadata": {},
   "source": [
    "## Bayesian Modeling of Radon Concentration      \n",
    "\n",
    "In this chapter, we will use the prediction of radon concentration in counties of Minnesota as a running example for exploring and comparing hierarchical Bayesian models. We will construct and compare three models:    \n",
    "1. A pooled model with a single intercept and slope for all counties.    \n",
    "2. An unpooled model with separate intercepts and slopes for each county.   \n",
    "3. A hierarchical model with hyper-priors for the behavior of all counties and at the next level the slopes and intercepts for each county computed using the hyper-priors.   \n",
    "\n",
    "***********\n",
    "Note: This notebook is based in part on [this example](https://docs.pymc.io/en/v3/pymc-examples/examples/generalized_linear_models/GLM-hierarchical.html) from the PyMC documentation.   \n",
    "***********\n",
    "\n",
    "To get started execute the code in the cell below to import the packages required for the examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f897c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import xarray as xr\n",
    "import scipy.stats as spst\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a915b9",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to load the dataset and display the head of the data frame.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae797ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "radon_data = pd.read_csv(pm.get_data(\"radon.csv\"))\n",
    "county_names = radon_data.county.unique()\n",
    "radon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8a395",
   "metadata": {},
   "source": [
    "The response variable is the log of radon concentration. The independent variable is the `floor` indicator for the presence of a basement.\n",
    "\n",
    "In order to construct the PyMC models, we need to create a dictionary with the counties and county names and indices. Execute the code in the cell below to create the dictionary and display a sample of the county index numbers.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_idxs, counties = pd.factorize(radon_data.county)\n",
    "coords = {\n",
    "    \"county\": counties,\n",
    "    \"obs_id\": np.arange(len(county_idxs)),\n",
    "}\n",
    "print('county_idxs = ' + str(county_idxs[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36f256",
   "metadata": {},
   "source": [
    "Notice that there are several samples for each county.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0574a35",
   "metadata": {},
   "source": [
    "## Pooled Model\n",
    "\n",
    "The first model we will examine is a **pooled model**. The pooled model is a linear regression model of the log of Radon concentration using single slope and intercept coefficients. The coefficient values are computed from the data pooled over all of the counties, hence the name. You can think of this model as flat, with respect to county. In other words, we assume the observations within each county are **[exchangeable](https://en.wikipedia.org/wiki/Exchangeable_random_variables)** with each other. Exchangeable values are assumed to be iid and therefore have the same variance.      \n",
    "\n",
    "Typically for a regression model the pooled model uses a Normal likelihood model:    \n",
    "\n",
    "\\begin{align}\n",
    "log(radon) &\\sim \\mathtt{N}(\\mu_c, \\epsilon)\\\\ \n",
    "where\\\n",
    "\\mu_c &= \\beta x_c  \n",
    "\\end{align}\n",
    "\n",
    "The value of $\\mu$ is computed **deterministically** using the model coefficient vector, $\\beta$, and the vector of independent variable values, $x$. \n",
    "\n",
    "The structure of the model is flat with just parameters $[\\beta, \\epsilon]$. These parameters have prior distributions:     \n",
    "\n",
    "\\begin{align}   \n",
    "\\beta &\\sim \\mathtt{N}(0, \\sigma)\\\\ \n",
    "\\epsilon &\\sim |\\mathtt{Cauchy}(\\eta)\\\\\n",
    "\\end{align}   \n",
    "\n",
    "Where, $|\\mathtt{Cauchy}$ is the half Cauchy distribution with one parameter, $\\eta$. This distribution is often used for variance priors since it has heavy tails and no values $< 0$. To get a feel for this distribution execute the code in the cell below.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b13e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('arviz-darkgrid')\n",
    "x = np.linspace(0, 5, 200)\n",
    "for b in [0.5, 1.0, 2.0, 5.0]:\n",
    "    pdf = spst.cauchy.pdf(x, scale=b)\n",
    "    plt.plot(x, pdf, label=r'$\\eta$ = {}'.format(b))\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6f622",
   "metadata": {},
   "source": [
    "Notice that the value of the half Cauchy is at a maximum of 0. As the value of $\\eta$ increases, the dispersion of the distribution increases.     \n",
    "\n",
    "We can repersent this **flat Bayesian model** as a directed acyclic graph (DAG), as shown below. The single likelihood model for all the observations is at the bottom level. The prior distributions for the parameters are at the top level.     \n",
    "\n",
    "<img src=\"../images/FlatBayesDAG.PNG\" alt=\"Drawing\" style=\"width:600px; height:200px\"/>\n",
    "<center> DAG of Flat Bayesian Model <center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63865e8",
   "metadata": {},
   "source": [
    "### Defining and sampling the model\n",
    "\n",
    "The PyMC code for the model is shown in the cell below. Read the comments to understand the code. Execute this code to instantiate the model object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dcf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as pooled_model:\n",
    "    # Independent variable is just number of floors   \n",
    "    floor = pm.Data(\"floor\", radon_data.floor.values, mutable=True)   \n",
    "    # Priors for unknown model parameters\n",
    "    betas = pm.Normal(\"betas\", mu=0, sigma=100, shape=2)\n",
    "    sigma = pm.HalfCauchy(\"sigma\", 5)\n",
    "\n",
    "    # Deterministic expected value of outcome\n",
    "    radon_est = betas[0] + betas[1] * floor\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    y = pm.Normal(\"y\", mu=radon_est, sigma=sigma, observed=radon_data.log_radon) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c32a5",
   "metadata": {},
   "source": [
    "### Prior predictive checks\n",
    "\n",
    "With the pooled model defined, we can perform prior predictive checks on the model. The code in the cell below performs the prior predictive sampling of the model and displays the density of the Betas.  Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3434\n",
    "with pooled_model:\n",
    "    pooled_prior_checks = pm.sample_prior_predictive(samples=500, random_seed=SEED)\n",
    "\n",
    "_,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "for i in range(2): \n",
    "    sns.kdeplot(pooled_prior_checks['prior'].betas[0,:,i], ax=ax[0]);\n",
    "    ax[0].set_title('Density of Betas');   \n",
    "    \n",
    "sns.kdeplot(pooled_prior_checks['prior'].sigma[0,:], ax=ax[1]);\n",
    "ax[1].set_title('Density of Sigma');    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42aca4",
   "metadata": {},
   "source": [
    "The density of the Beta priors looks reasonable, but with a wide dispursion.   \n",
    "\n",
    "You can now MCMC sample the model by executing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed40a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pooled_model:\n",
    "    pooled_trace = pm.sample(2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93875f",
   "metadata": {},
   "source": [
    "The quality of the sampling needs to be verified. To get started, execute the code in the cell below to display the trance plots, by executing the code in the cell below.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab130de",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(pooled_trace, combined=False, var_names=[\"betas\", \"sigma\"]); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ddd6b",
   "metadata": {},
   "source": [
    "Examine these plots. The traces and the density plots look nearly the same for the different traces. Further, the dispersion is greatly srunk compared to the prior predictive densities. This is promising, indicating good convergence of the MCMC sampling.        \n",
    "\n",
    "The next step is to check the ESS for the parameters. Execute the code in the cell below to display the sampling summary table.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(pooled_trace, kind='diagnostics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564af15f",
   "metadata": {},
   "source": [
    "Examine the summary table. All of these performance statistics look reasonable, given the number of samples for the trances          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7a0fd",
   "metadata": {},
   "source": [
    "### Inference on the model parameters       \n",
    "\n",
    "With the MCMC sampling completed, inference can now be performed on the model parameters. Execute the code in the cell below to display the forest plot for the model parameters.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(pooled_trace, var_names=[\"betas\", \"sigma\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872b059",
   "metadata": {},
   "source": [
    "We can make some simple inferences about the parameters of this model.    \n",
    "1. The HDI of $\\beta_0$ the intercept and $\\sigma$ are in a relatively narrow range.   \n",
    "2. The HDI of the slope parameter, $\\beta_1$, is considerably larger than the other parameters. This indicates that the effect size for the home having a basement or not, is poorly determined.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0558fd",
   "metadata": {},
   "source": [
    "### Posterior predictive checks    \n",
    "\n",
    "Finally, we must perform some posterior predictive checks. We must verify that realizations of predicted values from the posterior distribution agree with the distribution of the actual observations. To do so, execute the code in the cell below and examine the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 6545\n",
    "with pooled_model:\n",
    "    pm.sample_posterior_predictive(pooled_trace, extend_inferencedata=True, random_seed=SEED)\n",
    "_,ax = plt.subplots(3,1, figsize=(8,9))\n",
    "az.plot_ppc(pooled_trace, ax=ax[0]);\n",
    "az.plot_bpv(pooled_trace, kind='p_value', ax=ax[1]);\n",
    "az.plot_bpv(pooled_trace, kind='u_value', ax=ax[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49853b13",
   "metadata": {},
   "source": [
    "Examine the plots, noting the following:    \n",
    "1. The agreement between the observed and the realizations simulated from the posterior distribution of the response variable is generally good in the middle of the distribution. There is one deviation in that the peak of the observed values is just to the right of the maximum of the realized values. Further, the agreement between the posterior and observed shows a deviation in both tails.     \n",
    "2. The distribution of the Bayesian p-values shows a skew with respect to the ideal distribution. This indicates some systematic error in the posterior distribution.     \n",
    "3. The Bayesian u-values deviate a bit from the ideal 1.0, but stay within the desired credible interval, indicating any bias in the model is not significant.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ec050",
   "metadata": {},
   "source": [
    "## Unpooled model    \n",
    "\n",
    "We will now move on to the unpooled model. The unpooled model treats each county independently, with separate coefficients. In more technical terms, the county is the unit of **exchangeability** for the unpooled model. Any county can be exchanged with a common variance. The unpooled model is at the other extreme when compared to the pooled model which has a single set of model parameters with the observations as the unit of exchangeability. \n",
    "\n",
    "For the unpooled model, we will use the subscript $c$ to indicate county. The likelihood for the unpooled model can be written:    \n",
    "\n",
    "\\begin{align}   \n",
    "log(radon_c) &\\sim \\mathtt{N}(\\mu_c, \\epsilon)\\\\ \n",
    "where\\\n",
    "\\mu_c &= \\beta_c x_c  \n",
    "\\end{align}\n",
    "\n",
    "The value of $\\mu_c$ is computed **deterministically** using the model coefficient vector, $\\beta_c$, and the vector of independent variable values, $x_c$. For unpooled models, there are separate vectors $\\beta_c$ and $x_c$ for each county.      \n",
    "\n",
    "The structure of the model for a county is flat with parameters $[\\beta_c, \\epsilon]$. These parameters have prior distributions:     \n",
    "\n",
    "\\begin{align}   \n",
    "\\beta_c &\\sim \\mathtt{N}(0, \\sigma)\\\\ \n",
    "\\epsilon &\\sim |\\mathtt{Cauchy}(\\eta)\\\\\n",
    "\\end{align}   \n",
    "\n",
    "We use the same priors for all counties.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130da2c5",
   "metadata": {},
   "source": [
    "As with the pooled Bayes model we can represent the unpooled Bayes model as a DAG, as shown below. The difference here is that the model has a different location parameter, $\\mu_C$, for each county, $C$. This difference is why the model is considered **unpooled**. The same prior distributions of the parameters are used for each county.   \n",
    "\n",
    "<img src=\"../images/UnpooledBayesDAG.png\" alt=\"Drawing\" style=\"width:600px; height:200px\"/>\n",
    "<center> DAG of Unpooled Bayesian Model <center> \n",
    "    \n",
    "> **Plate notation:** The rectangle shown around the likelihood is known as a **plate**. The plate is used to represent the fact that there are, in fact, multiple distributions. In this case, the plate indicates that there is one likelihood for each county, $C$.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b090dc1",
   "metadata": {},
   "source": [
    "### Define and sample the model     \n",
    "\n",
    "The code in the cell below defines the unpooled PyMC model. Notice that the model parameters now have dimensions of the number of counties, except for the error term, $\\epsilon$. The likelihood is now vector (multiple) valued on a county-by-county basis. More details can be seen in the code and comments. Execute the code in the cell to instantiate the model.                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e73085",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as unpooled_model:\n",
    "\n",
    "    # Independent parameters for each county\n",
    "    county_idx = pm.Data(\"county_idx\", county_idxs, dims=\"obs_id\")\n",
    "    floor = pm.Data(\"floor\", radon_data.floor.values, dims=\"obs_id\", mutable=True)\n",
    "\n",
    "    Beta0 = pm.Normal(\"Beta0\", 0, sigma=100, dims=\"county\")\n",
    "    Beta1 = pm.Normal(\"Beta1\", 0, sigma=100, dims=\"county\")\n",
    "\n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy(\"eps\", 5)\n",
    "\n",
    "    # Model prediction of radon level\n",
    "    # BetaX[county_idx] translates to a[0, 0, 0, 1, 1, ...],\n",
    "    # we thus link multiple household measures of a county\n",
    "    # to its coefficients.\n",
    "    radon_est = Beta0[county_idx] + Beta1[county_idx] * floor\n",
    "\n",
    "    # Data likelihood\n",
    "    y = pm.Normal(\"y\", radon_est, sigma=eps, observed=radon_data.log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51fd91",
   "metadata": {},
   "source": [
    "### Prior predictive checks\n",
    "\n",
    "With the model defined we can perform some prior predictive checks. Tje process is nearly the same as for the pooled model There are now coeficients for each county. We will only display plots for the first 20. Execute the code in the cell below and examine the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3434\n",
    "with unpooled_model:\n",
    "    unpooled_prior_checks = pm.sample_prior_predictive(samples=500, random_seed=SEED)\n",
    "\n",
    "_,ax = plt.subplots(2,2,figsize=(10,4))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(20): \n",
    "    sns.kdeplot(unpooled_prior_checks['prior'].Beta0[0,:,i], ax=ax[0]);\n",
    "    ax[0].set_title('Density of Beta0');    \n",
    "for i in range(20): \n",
    "    sns.kdeplot(unpooled_prior_checks['prior'].Beta1[0,:,i], ax=ax[1]);\n",
    "    ax[1].set_title('Density of Beta1');    \n",
    "    \n",
    "sns.kdeplot(unpooled_prior_checks['prior'].eps[0,:], ax=ax[2]);\n",
    "ax[2].set_title('Density of eps');    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf672f7",
   "metadata": {},
   "source": [
    "THe densities are very similar to those of the pooled model. There is little difference in the parameters a or b between counties.   \n",
    "\n",
    "The code in the cell below samples the PyMC model. Given the higher dimensionality of the problem (larger number of parameters) we use 2,000 MCMC samples after the burn-in period. Execute this code.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d47ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with unpooled_model:\n",
    "    unpooled_trace = pm.sample(2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d445e",
   "metadata": {},
   "source": [
    "The next step is to evaluate the trace plots. Execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442aa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(unpooled_trace, var_names=['Beta0','Beta1','eps'], combined=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe380717",
   "metadata": {},
   "source": [
    "There is a posterior distribution of each of the intercept and slope parameters for each of the counties, 85 of each. There is only one deviation parameter, $\\epsilon$ common to all the counties. Keeping the structure of the model in mind, examine these plots noting the following:   \n",
    "1. From the density plots of the intercept, $a$, it is clear that there is a considerable difference between the distributions for the counties. There appears to be good agreement  With $n_chains \\times 85$ traces the plots are hard to interpret. In general, the trace plots show no abnormalities.       \n",
    "2. The range of the slope parameter values, $b$, is quite wide. Most of the values are in a narrow range but with some distributions with a wide range.  Again, the trace plot is hard to interpret but shows no particular problem.   \n",
    "3. The density plots for the dispersion parameter, $\\epsilon$, show a reasonable range of values and are similar between the traces. The trace plot shows no particular problem, with good agreement between the traces.   \n",
    "4. The wide range of intercept and slope parameter values is consistent with an unpooled model. For some counties, only a few observations are used to estimate these parameters.   \n",
    "\n",
    "Next, execute the code in the cell below to see a summary table of the sampling statistics.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To see all rows of the table, uncomment the line of code below and execute. \n",
    "az.summary(unpooled_trace, kind='diagnostics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b145976-05b7-4ff8-8cef-d3b59bfd112b",
   "metadata": {},
   "source": [
    "Examine this table and notice the following:   \n",
    "1. The MCSE and bulk ESS values are reasonable for the intercept parameter, $a$. As might be expected, the tail ESS values are significantly less than the Bulk ESS, but not so much as to cause problems.   \n",
    "2. The MCSE values for the slope parameter, $b$, vary over two orders of magnitude. This is consistent with the wide range of slope values seen in the posterior density plots. The bulk ESS values are reasonable. As might be expected, the tail ESS values are significantly less than the Bulk ESS, as a result of the wide tails on some of the posterior distributions.   \n",
    "3. The MSCE for the dispersion parameter $\\epsilon$ is quite small. The bulk ESS and tail ESS are lower than for the other parameters, but not so low as to cause serious problems.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5767ea",
   "metadata": {},
   "source": [
    "### Posterior predictive checks    \n",
    "\n",
    "Finally, we will perform some posterior predictive checks on the unpooled model. Execute the code in the cell below to display the graphs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 6567\n",
    "with unpooled_model:\n",
    "    pm.sample_posterior_predictive(unpooled_trace, extend_inferencedata=True, random_seed=SEED)\n",
    "_,ax = plt.subplots(3,1, figsize=(8,9))\n",
    "az.plot_ppc(unpooled_trace, ax=ax[0]);\n",
    "az.plot_bpv(unpooled_trace, kind='p_value', ax=ax[1]);\n",
    "az.plot_bpv(unpooled_trace, kind='u_value', ax=ax[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aef6b6-b225-4ed7-9d70-dc0c9755a859",
   "metadata": {},
   "source": [
    "> **Exercise 30-1:** Examine the plots and compare them to the plots for the pooled model. Answer the following quesitons:     \n",
    "> 1. Compare the posterior predictive mean to the observed distribution of the pooled and unpooled models?         \n",
    "> 2. Copare the Nayesian p-value curves between the pooled and unpooled models?      \n",
    "> 3. Wjat can you observe about the Bayesian U-value of the unpooled model and what does this tell you about the fit of that model?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43affc0-dec0-4636-a34f-3cb1507d93db",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.                    \n",
    "> 2.              \n",
    "> 3.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24a149",
   "metadata": {},
   "source": [
    "## Hierarchical Model\n",
    "\n",
    "We will now turn our attention to a hierarchical model. The hierarchical model is an intermediate between a pooled model and an unpooled model. As a result, hierarchical models are sometimes called **partial pooled models**.   \n",
    "\n",
    "Working from the top of the hierarchy downward, the construction of the model requires the following:      \n",
    "1. Definition of **hyperprior distributions** at the top level of the hierarchy. The hyperpriors can be thought of as overall priors for all parameters, regardless of category. In this case, we have one set of hyperpriors for all counties for the intercept and slope parameters. The use of hyperpriors is the key difference between the partially pooled hierarchical model and the unpooled model.       \n",
    "2. Prior distributions are defined for each parameter for each category. These priors use the hyperpriors as their prior distributions. The dimensionality of these priors is the same as for the unpooled model. In contrast, a pooled model has only a single prior for each parameter.           \n",
    "3. A multivalued (vector-valued) likelihood model, using the priors, is defined. \n",
    "\n",
    "At the bottom level of the hierarchy, a likelihood model is defined for each county:    \n",
    "\n",
    "\\begin{align}   \n",
    "log(radon_c) &\\sim \\mathtt{N}(\\mu_c, \\epsilon)\\\\ \n",
    "where\\\n",
    "\\mu_c &= \\beta_c x_c  \n",
    "\\end{align}\n",
    "\n",
    "The value of $\\mu_c$ is computed **deterministically** using the model coefficient vector, $\\beta_c$, and the vector of independent variable values, $x_c$. For the hierarchical models, there are separate vectors $\\beta_c$ and $x_c$ for each county.      \n",
    "\n",
    "The structure of the model for a county is flat with parameters $[\\beta_c, \\epsilon]$. These parameters have prior distributions:     \n",
    "\n",
    "\\begin{align}   \n",
    "\\beta_c &\\sim \\mathtt{N}(\\beta_h, \\epsilon_h)\\\\ \n",
    "\\epsilon &\\sim |\\mathtt{Cauchy}(\\eta)\\\\\n",
    "\\end{align}   \n",
    "\n",
    "Notice that the prior for the dispersion parameter $\\epsilon$ is the same for all counties. As with the pooled model.        \n",
    "\n",
    "Finally, at the top level of the hierarchy, we define the hyperpriors used to estimate $\\beta_c$. These hyperpriors are the same for all cases, all counties.    \n",
    "\n",
    "\\begin{align}    \n",
    "\\beta_h &\\sim \\mathtt{N}(0, \\sigma)\\\\ \n",
    "\\epsilon_h &\\sim |\\mathtt{N}(0, \\eta)\\\\\n",
    "\\end{align}    \n",
    "\n",
    "The hierarchical structure of the model can be illustrated by a DAG, as shown below. At the root of the graph is the likelihood of the model. The plate notation shows that there is a separate likelihood model for each country. The priors of the likelihood model are shown at the next level up. The plat notation shows that there is a separate prior for each of the countries. The **hyperpriors** for the parameters of the priors are shown at the top level. There is only one hyperprior distribution for each parameter.   \n",
    "\n",
    "<img src=\"../images/HeirarchicalBayesDAG.png\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center> DAG of Hierarchicall Bayesian Model <center> \n",
    "\n",
    "Execute the code in the cell below to instantiate this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6747bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as hierarchical_model:\n",
    "    county_idx = pm.Data(\"county_idx\", county_idxs, dims=\"obs_id\", mutable=True)\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_0 = pm.Normal(\"mu_0\", mu=0.0, sigma=20)\n",
    "    sigma_0 = pm.HalfNormal(\"sigma_0\", 5.0)\n",
    "    mu_1 = pm.Normal(\"mu_1\", mu=0.0, sigma=20)\n",
    "    sigma_1 = pm.HalfNormal(\"sigma_1\", 5.0)\n",
    "\n",
    "    # Intercept for each county, distributed around group mean mu_a\n",
    "    # Above we just set mu and sd to a fixed value while here we\n",
    "    # plug in a common group distribution for all a and b (which are\n",
    "    # vectors of length n_counties).\n",
    "    Beta0 = pm.Normal(\"Beta0\", mu=mu_0, sigma=sigma_0, dims=\"county\")\n",
    "    # effect difference between basement and floor level\n",
    "    Beta1 = pm.Normal(\"Beta1\", mu=mu_1, sigma=sigma_1, dims=\"county\")\n",
    "\n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy(\"eps\", 5.0)\n",
    "\n",
    "    radon_est = Beta0[county_idx] + Beta1[county_idx] * radon_data.floor.values\n",
    "\n",
    "    # Data likelihood\n",
    "    radon_like = pm.Normal(\"radon_like\", mu=radon_est, sigma=eps, observed=radon_data.log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f7ebb",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to perform MCMC sampling from the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f00af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(4000, tune=4000, target_accept=0.95, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2eb292",
   "metadata": {},
   "source": [
    "Notice the warning messages produced indicating there is a problem with the hierarchical model. The acceptance rate has already been raised to 0.95 and the number of samples for burn-in and posterior sampling set to 4000. It is likely that we need a different model.   \n",
    "\n",
    "Execute the code in the cell below to display the trace plot to further explore the problems with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04829cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(hierarchical_trace, var_names=['mu_0', 'sigma_0', 'mu_1', 'sigma_1', 'Beta0','Beta1','eps'], combined=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ab0c3",
   "metadata": {},
   "source": [
    "Examine the density plots. Notice the divergences of the $\\sigma_b$ densities. These divergences indicate a problem with the prior distribution of the dispersions.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4331a",
   "metadata": {},
   "source": [
    "### Defining an improved model    \n",
    "\n",
    "There are several observations we can make to help us define a better model.    \n",
    "1. The posterior predictive analysis for both the pooled and unpooled models shows the likely presence of outliers in the observations. A sensible and commonly applied approach to creating robust Bayesian models is to use a heavy-tailed distribution, such as the Student T distribution. The heavy-tailed distribution allows for outliers in the sampling of the posterior distributions.        \n",
    "2. The Cauchy distribution used for the dispersion parameter is perhaps not ideal, since the maximum is at 0. From even simple exploration of the data, it is clear the dispersion of the posterior distributions is far from 0. A commonly used alternative is the inverse gamma distribution. Unlike the Cauchy and Half Normal, the inverse gamma distribution is 0 at 0. Like the Cauchy, the inverse Gamma has a long right tail.   \n",
    "\n",
    "To display the general shape of the Student T distribution for several parameter values, execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.style.use('seaborn-darkgrid')\n",
    "x = np.linspace(-8, 8, 200)\n",
    "mus = [0., 0., 0., 0.]\n",
    "sigmas = [1., 1., 5., 5.]\n",
    "dfs = [1., 10., 1., 10.]\n",
    "for mu, sigma, df in zip(mus, sigmas, dfs):\n",
    "    pdf = spst.t.pdf(x, df, loc=mu, scale=sigma)\n",
    "    plt.plot(x, pdf, label=r'$\\mu$ = {}, $\\sigma$ = {}, $\\nu$ = {}'.format(mu, sigma, df))\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5eab76",
   "metadata": {},
   "source": [
    "Examine this plot. The dispersion of the Student T distribution increases with the value of $\\sigma$ and decreases with the degrees of freedom, $\\nu$. To create a robust model with will use a prior with larger $\\sigma$ and smaller $\\nu$.       \n",
    "\n",
    "To gain some feel for the shape of the inverse Gamma distribution of several parameter choices execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab55f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 500)\n",
    "alphas = [1., 2., 1., 2.]\n",
    "betas = [2., 2., 1., 1.]\n",
    "for a, b in zip(alphas, betas):\n",
    "    pdf = spst.invgamma.pdf(x, a, scale=b)\n",
    "    plt.plot(x, pdf, label=r'$\\alpha$ = {}, $\\beta$ = {}'.format(a, b))\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ca73c",
   "metadata": {},
   "source": [
    "Notice how the shape of the inverse gamma distribution changes with the parameters. The model shifts right with larger values of $\\alpha$. The right tail becomes heavier with larger values of $$\\beta. \n",
    "\n",
    "With these new choices for hyperpriors and the response in mind, we can update our definition of the hierarchical model. At the bottom level of the hierarchy, we use a Student T likelihood model. The Student T distribution better accounts for the outliers in the response variable.      \n",
    "\n",
    "\\begin{align}   \n",
    "log(radon_c) &\\sim StudentT(\\mu_c, \\epsilon, \\nu)\\\\ \n",
    "where\\\\\n",
    "\\mu_c &= \\beta_c x_c  \n",
    "\\end{align}\n",
    "\n",
    "As before, the value of $\\mu_c$ is computed **deterministically** using the model coefficient vector, $\\beta_c$, and the vector of independent variable values, $x_c$. For the hierarchical models, there are separate vectors $\\beta_c$ and $x_c$ for each county.      \n",
    "\n",
    "The structure of the model for a county is flat with parameters $[\\beta_c, \\epsilon]$. These parameters have the same prior distributions as before:     \n",
    "\n",
    "\\begin{align}   \n",
    "\\beta_c &\\sim \\mathtt{N}(\\beta_h, \\epsilon_h)\\\\ \n",
    "\\epsilon &\\sim |\\mathtt{Cauchy}(\\eta)\\\\\n",
    "\\end{align}   \n",
    "\n",
    "Notice that the prior for the dispersion parameter $\\epsilon$ is the same for all counties. As with the pooled model.        \n",
    "\n",
    "Finally, at the top level of the hierarchy, we define new hyperpriors to estimate $\\beta_c$. These hyperparameters differentiate this model from the first hierarchical model. These hyperpriors are the same for all cases, all counties.    \n",
    "\n",
    "\\begin{align}    \n",
    "\\beta_h &\\sim StudentT(0, \\sigma, \\nu)\\\\ \n",
    "\\epsilon_h &\\sim InverseGamma(0, \\eta, \\kappa)\\\\\n",
    "\\end{align}    \n",
    "\n",
    "We can represent this improved hierarchical model with the DAG shown below.   \n",
    "\n",
    "<img src=\"../images/HeirarchicalBayesDAG2.png\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center> DAG of Improved Hierarchicall Bayesian Model <center> \n",
    "\n",
    "Execute the code in the cell below to instantiate the new hierarchical model.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b400e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as hierarchical_model:\n",
    "    county_idx = pm.Data(\"county_idx\", county_idxs, dims=\"obs_id\", mutable=True)\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_0 = pm.StudentT(\"mu_0\", mu=0.0, sigma=20.0, nu=5)\n",
    "    sigma_0 = pm.InverseGamma(\"sigma_0\",alpha=1.0, beta=2.0)\n",
    "    mu_1 = pm.StudentT(\"mu_1\", mu=0.0, sigma=20.0, nu=5)\n",
    "    sigma_1 = pm.InverseGamma(\"sigma_1\", alpha=1.0, beta=2.0)\n",
    "\n",
    "    # Intercept for each county, distributed around group mean mu_a\n",
    "    # Above we just set mu and sd to a fixed value while here we\n",
    "    # plug in a common group distribution for all a and b (which are\n",
    "    # vectors of length n_counties).\n",
    "    Beta0 = pm.Normal(\"Beta0\", mu=mu_0, sigma=sigma_0, dims=\"county\")\n",
    "    # effect difference between basement and floor level\n",
    "    Beta1 = pm.Normal(\"Beta1\", mu=mu_1, sigma=sigma_1, dims=\"county\")\n",
    "\n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy(\"eps\", 5.0)\n",
    "\n",
    "    radon_est = Beta0[county_idx] + Beta1[county_idx] * radon_data.floor.values\n",
    "\n",
    "    # Data likelihood\n",
    "    radon_like = pm.StudentT(\"radon_like\", mu=radon_est, sigma=eps, nu=5, observed=radon_data.log_radon, dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a79995-9877-4338-bae0-1ccfd39d5c8d",
   "metadata": {},
   "source": [
    "> **Exercise 30-2:** Examine the structure of the hierarchical model and answer the following questions.      \n",
    "> 1. How is the unpooled model different from the pooled model?\n",
    "> 2. Considering the principle of exchangeability, what is a key similarity between the unpooled and hierarchical models?        \n",
    "> 3. How do the hyperparameters relate to the observations across counties?         \n",
    "> 4. How do the number of priors and number of counties relate to each other?\n",
    "> 5. How can you relate the principle of exchangeability to the set of likelihoods?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9bfa6-5430-4980-b40d-172f458242f8",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.       \n",
    "> 2. \n",
    "> 3.        \n",
    "> 4.               \n",
    "> 5.               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703b3e8",
   "metadata": {},
   "source": [
    "### Prior predictive checks      \n",
    "\n",
    "Often with hierarchical models, the interactions of the hyperpriors may not be intuitive. Therefoe, before going any further we should check the hyperprior distributions for the new hierarchical model.   \n",
    "\n",
    "Execute the code in the cell below to sample the prior distributions.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3434\n",
    "with hierarchical_model:\n",
    "    hierarchical_prior_checks = pm.sample_prior_predictive(samples=100, random_seed=SEED)\n",
    "hierarchical_prior_checks.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659d457",
   "metadata": {},
   "source": [
    "With the prior predictive sampling complete, execute the code in the cell below to display the hyperprior densities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax=plt.subplots(3,2,figsize=(10,6))\n",
    "ax=ax.flatten()\n",
    "for i,var in enumerate(zip([hierarchical_prior_checks['prior'].Beta0.to_numpy().flatten(), \n",
    "                        hierarchical_prior_checks['prior'].Beta1.to_numpy().flatten(),\n",
    "                        hierarchical_prior_checks['prior'].mu_0.to_numpy().flatten(), \n",
    "                        hierarchical_prior_checks['prior'].mu_1.to_numpy().flatten(),\n",
    "                        hierarchical_prior_checks['prior'].sigma_0.to_numpy().flatten(), \n",
    "                        hierarchical_prior_checks['prior'].sigma_1.to_numpy().flatten()],\n",
    "                        ['Beta0','Beta1','mu_0','mu_1','sigma_0','sigma_1'])):\n",
    "    sns.kdeplot(var[0], ax=ax[i]);\n",
    "    ax[i].set_title(var[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6a530",
   "metadata": {},
   "source": [
    "The hyperprior distributions, `a` and `b`, look reasonable given what we know so far from our analysis of this problem. The range of values seems sensible, and are sufficient to not overly constrain the solution. The parameter priors look wide enough to not restrict the solution as well. Note that `sigma_a` and `sigma_b` will not have values less than 0, but the KDE kernel does not cut-off at 0.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f46ad",
   "metadata": {},
   "source": [
    "### MCMC sampling       \n",
    "\n",
    "With the model defined and the hyperpriors investigated we are ready to MCMC sample the model. Execute the code in the cell below to do so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(4000, tune=4000, target_accept=0.95, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6e7d9",
   "metadata": {},
   "source": [
    "With the new model defined the divergence warnings have been eliminated. Still, the sampling is not ideal. You can see the warning about low ESS. With an ESS of about 400 for some parameters, the sampling may still be adequate if not ideal.     \n",
    "\n",
    "To further investigate the sampling, execute the code in the cell below to display the trace plots.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25568c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(hierarchical_trace, var_names=['mu_0', 'sigma_0', 'mu_1', 'sigma_1', 'Beta0','Beta1','eps'], combined=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a5f77",
   "metadata": {},
   "source": [
    "Compare these trace plots to the previous hierarchical model. You can see that the traces and densities for $\\sigma_b$ now converge. Further, you can see that the posterior density of the slope parameters, $b$, are more varied for the different counties. Overall, these trace plots show a reasonable sampling of the model parameters.      \n",
    "\n",
    "Next, execute the code in the cell below to display the summary table of sampling statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(hierarchical_trace, kind='diagnostics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f976c9d",
   "metadata": {},
   "source": [
    "Overall the MCSE and ESS values are reasonable. The exceptions are the bulk and tail ESS of the hyperparamers, particularly $b$ and $\\sigma_b$. Still, overall the ESS looks sufficient, if not ideal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58a7ea",
   "metadata": {},
   "source": [
    "### Inference on the model       \n",
    "\n",
    "With a reasonable sampling of the hierarchical model, we are in a position to perform some inference on the model. Execute the code in the cell below to display the forest plot of the model parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(hierarchical_trace, var_names=['mu_0', 'sigma_0', 'mu_1', 'sigma_1','Beta0','Beta1','eps']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cca113-c265-4b9b-b65c-cedd4f6f9c0a",
   "metadata": {},
   "source": [
    "> **Exercise 30-3:** Examine the forest plot and answer the following questions:     \n",
    "> 1. Examine the HDIs of the county intercept parameters. Using the HDIs or credible intervals, can yuou identify cases of statistically significant differences?\n",
    "> 2. Can you identify statistically significant differences in the slope coefficients?     \n",
    "> 3. What do your foregoing inferences tell you about the sparial (county) changes in mean and slope (basement) effects in this model.          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45999b4c-0326-4be7-9fb6-8c248008b2d6",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.                  \n",
    "> 2.             \n",
    "> 3.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58386e0",
   "metadata": {},
   "source": [
    "### Posterior predictive checks    \n",
    "\n",
    "Finally, execute the code in the cell below to sample the posterior predictive distributions and display the charts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 6565\n",
    "with hierarchical_model:\n",
    "    pm.sample_posterior_predictive(hierarchical_trace, extend_inferencedata=True, random_seed=SEED) \n",
    "\n",
    "_,ax = plt.subplots(3,1, figsize=(8,9))    \n",
    "az.plot_ppc(hierarchical_trace, num_pp_samples=100, ax=ax[0]);    \n",
    "az.plot_bpv(hierarchical_trace, kind='p_value', ax=ax[1]);\n",
    "az.plot_bpv(hierarchical_trace, kind='u_value', ax=ax[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0861634",
   "metadata": {},
   "source": [
    "Examine these plots noticing the following:      \n",
    "1. The realizations of the response variables are in reasonable agreement with the observations, with a few noticeable deviations. As a result of using the Student T distribution for the response, the tails of the realized distribution are noticeably heavier and more consistent with the observations.        \n",
    "2. The Bayesian p-values are closer to the ideal distribution than either the pooled or unpooled models.      \n",
    "3. There is the deviation of the Bayesian u-values from ideal behavior in the tails of the posterior distribution. These deviations are reduced with respect to the unpooled model.   \n",
    "\n",
    "In summary, the posterior predictive distribution of the hierarchical model is an improvement over the pooled or unpooled model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f36e4",
   "metadata": {},
   "source": [
    "## Comparing Models   \n",
    "\n",
    "We have now constructed and independently evaluated three Bayesian models. The question now is how can we compare the performance of these models. The method used is based on a **leave one out (LOO)** **cross-validation (CV)** algorithm. This method is a variation on the jackknife resampling method briefly introduced in Chapter 14.          \n",
    "\n",
    "Specifically, the **expected log pointwise density (ELPD)** is a measure of deviance (see Chapter 22). The ELPD cannot be computed directly since we do not know the true probability density of the response variable, $p(y)$. Instead, we use an approximation using the likelihood and the posterior density. For $n$ observations $y_i$ and model parameters, $\\theta$, we can write the ELPD:           \n",
    "\n",
    "$$ELPD_{LOO_CV} = \\sum_{i=1}^n log \\int p(y_i\\ |\\ y_{-i}, \\theta)\\ p(\\theta\\ |\\ y_{-i}) d \\theta$$   \n",
    "\n",
    "Here:   \n",
    "$y = $ the set of observations.\n",
    "$y_{-i} = $ the set of observations less the ith.     \n",
    "$p(y_i\\ |\\ y_{-1}, \\theta) = $ the likelihood of the ith observation, computed with the ith observation help back.    \n",
    "$p(\\theta\\ |\\ y_{-i}) = $ the posterior distribution computed excluding the ith observation. \n",
    "\n",
    "At first, this relationship does not seem intuitive. To gain some understanding consider the terms inside the integral. The first term is the likelihood of the held-back observation from the distribution comptued without the held back observation. This likelihood is weighted by the posterior probability of the model parameters, given the data used to fit the model, with the observation held back. The integral is the density at the held back observation of the posterior computed without the held back observation. \n",
    "\n",
    "$$p(y_i |\\  y_{-i})= \\int p(y_i\\ |\\ y_{-i}, \\theta)\\ p(\\theta\\ |\\ y_{-i}) d \\theta$$\n",
    "\n",
    "These densities are summed over the observations. So in summary, we can say that $ELPD_{LOO_CV}$ is sum of the densities with an observation held back.     \n",
    "\n",
    "The computational complexity of directly resampling the observations and recomputing the model would be prohibitive. Instead, an importance sampling algorithm known as Pareto smoothed importance sampling (PSIS) is used. The PSIS algorithm resamples from the already computed traces, making the computation of ELPD quite efficient.   \n",
    "\n",
    "You cabn find additional details on the interpretaton and compuation of $ELPD_{LOO_CV}$ in [this Vinette in the Stan documentation](https://mc-stan.org/loo/articles/loo2-non-factorized.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664343f6",
   "metadata": {},
   "source": [
    "There are a number of steps required to compute the $ELPD_{LOO_CV}$ for the three models. \n",
    "\n",
    "As a first step, we must compute the leave-one-out log-likelihood of each model we wish to compare. The code in the cell does this by these steps:   \n",
    "1. The log-likelihood of each model is computed from the sampled traces.    \n",
    "2. The leave-on-out samples of the log-likelihood are then computed.    \n",
    "\n",
    "Execute the code in the cell below.    \n",
    "\n",
    "> **Note:** The unpooled model is an insufficient sample, which will generate warning messages.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod, trace in zip([hierarchical_model,pooled_model,unpooled_model],[hierarchical_trace,pooled_trace,unpooled_trace]):\n",
    "    with mod:\n",
    "        pm.compute_log_likelihood(trace)\n",
    "\n",
    "for loo, trace in zip(['hierarchical_loo','pooled_loo','unpooled_loo'],[hierarchical_trace,pooled_trace,unpooled_trace]):\n",
    "    loo = az.loo(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de983e27",
   "metadata": {},
   "source": [
    "With the leave one out log-likelihood of each model computed the models can now be compared. Execute the code in the cell below to create and display the model comparison table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08859a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp_loo = az.compare({\"hierarchical\":hierarchical_trace, \"pooled\":pooled_trace, 'unpooled':unpooled_trace})\n",
    "df_comp_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728fd8b",
   "metadata": {},
   "source": [
    "The table is ordered by the rank of the model, with the best model shown as $0$. The other columns in the table are:       \n",
    "- *elpd_loo* is the $ELPD_{LOO_CV}$ of the model. Less negative values are better.   \n",
    "- *p_loo* is a pseudo estimate of the model complexity, an approximation of the number of parameters.   \n",
    "- *elpd_diff* is the difference in $ELPD_{LOO_CV}$ between the model and the best model.      \n",
    "- *weight* are weights used for model averaging.      \n",
    "- *se* is the standard error of the $ELPD_{LOO_CV}$.     \n",
    "- *dse* is the standard error of the elpd_diff.       \n",
    "- *warning* a warning that the $ELPD_{LOO_CV}$ estimate may not be reliable.    \n",
    "- *scale* the scale used to report $ELPD_{LOO_CV}$.   \n",
    "\n",
    "You can display a graphical representation of the $ELPD_{LOO_CV}$ and differences by executing the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce004182",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(8,4))\n",
    "az.plot_compare(df_comp_loo, insample_dev=False, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b643f-f406-4e26-9f68-8ffbc2a8c025",
   "metadata": {},
   "source": [
    "> **Exercise 30-4:** Examine the foregoing table and plot and answer the following quesitons:\n",
    "> 1. What the the relationship of the $ELPD_{LOO_CV}$ for the three models tell you about the fit to the data?     \n",
    "> 2. Is the relationship you discussed above for the best model significant and why?    \n",
    "> 3. Is the difference bwtween the other two models significantly different and why?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96929a",
   "metadata": {},
   "source": [
    "> **Answers:**         \n",
    "> 1.             \n",
    "> 2.            \n",
    "> 3.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8f1be",
   "metadata": {},
   "source": [
    "## Bayesian Shrinkage   \n",
    "\n",
    "We have already seen there is a significant difference between the hierarchical model and the other models. By imposing a hierarchy of priors on the model the values of the county intercept and slope are said to **shrink** toward the prior. In other words, using a hyperprior constrains the model fit and prevents poorly determined parameters from taking extreme values.  \n",
    "\n",
    "We have already seen evidence of parameter shrinkage in the hierarchical model from the fairly consistent HDIs of the model parameters. We will now explore the shrinkage of the parameters further.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512b920",
   "metadata": {},
   "source": [
    "As a first step, we must compute the MAP of the unpooled and hierarchical model parameters. Execute the code in the cell below to perform these operations.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d46e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with unpooled_model:\n",
    "    unpooled_MAP = pm.find_MAP()\n",
    "with hierarchical_model:\n",
    "    hierarchical_MAP = pm.find_MAP()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14510497",
   "metadata": {},
   "source": [
    "With MAP values of the parameters computed, we can examine the distribution of these parameters for the two models. To display density plots of the two model parameters execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_ax_ = plt.subplots(2,1,figsize=(10,7))\n",
    "for var,ax in zip(['Beta0','Beta1'],_ax_):\n",
    "    sns.kdeplot(unpooled_MAP[var], ax=ax, label='Unpooled')\n",
    "    sns.kdeplot(hierarchical_MAP[var], ax=ax, label='Hierarchical')\n",
    "    ax.set_title('Distributions of parameter ' + var + ' values')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524b8ac",
   "metadata": {},
   "source": [
    "Examine the foregoing plots. Notice that the density of the unpooled model parameters is much wider than that of the hierarchical model parameters. In other words, the hierarchical model parameter values are in a fairly narrow range compared to the unpooled model. This effect is an example of **Bayesian shrinkage** of the model parameters!    \n",
    "\n",
    "We can gain another perspective on Bayesian shrinkage by examining how the model parameters for each case, or county in this case, change between the unpooled and hierarchical models. To display this relationship, execute the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(\n",
    "    111,\n",
    "    xlabel=\"Parameter Beta0\",\n",
    "    ylabel=\"Parameter Beta1\",\n",
    "    title=\"Hierarchical vs. Unpooled Parameter MAP Values\",\n",
    "    xlim=(0, 3),\n",
    "    ylim=(-3, 3),\n",
    ")\n",
    "\n",
    "ax.scatter(unpooled_MAP[\"Beta0\"], unpooled_MAP[\"Beta1\"], s=25, alpha=0.4, label=\"Unpooled\")\n",
    "ax.scatter(hierarchical_MAP[\"Beta0\"], hierarchical_MAP[\"Beta1\"], c=\"red\", s=25, alpha=0.4, label=\"Hierarchical\")\n",
    "for i in range(len(unpooled_MAP[\"Beta0\"])):\n",
    "    ax.arrow(\n",
    "        unpooled_MAP[\"Beta0\"][i],\n",
    "        unpooled_MAP[\"Beta1\"][i],\n",
    "        hierarchical_MAP[\"Beta0\"][i] - unpooled_MAP[\"Beta0\"][i],\n",
    "        hierarchical_MAP[\"Beta1\"][i] - unpooled_MAP[\"Beta1\"][i],\n",
    "        fc=\"k\",\n",
    "        ec=\"k\",\n",
    "        length_includes_head=True,\n",
    "        alpha=0.2,\n",
    "        linewidth=1,\n",
    "        head_width=0.04,\n",
    "    )\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fcaef7",
   "metadata": {},
   "source": [
    "> **Exercise 30-5:** Examine the foregoing plot. The intercept value is on the horizontal axis and the floor (slope) is on the vertical axis. The blue dots show the parameter estimates for the unpooled model. The red dots show the coefficient values for the hierarchical model. The arrows connect the MAP coefficient values from the two models for the same county. Answer the following questions:          \n",
    "> 1. What does the wide dispersion of the MAP values of the unpooled model coefficients tell you about the fit of this model give the exchangability of the observations?     \n",
    "> 2. What do the closely packed coefficient MAP values of the hierarchical model tell you about the shrinkage of the coefficient values?\n",
    "> 3. Given your observations about shrinkage, what can you say about any need to regularize the hierarchical Bayesian model?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52eb7d-8e26-41c2-b81a-2fd61581f91b",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.              \n",
    "> 2.               \n",
    "> 3.          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44d748",
   "metadata": {},
   "source": [
    "#### Copyright 2022, 2023, 2024 Stephen F Elston. All rights reserved.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b6aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
